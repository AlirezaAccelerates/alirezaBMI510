% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bmi510.R
\name{precision}
\alias{precision}
\title{Calculate the precision of a classifier.}
\usage{
precision(pred, truth)
}
\arguments{
\item{pred}{A binary (logical) vector of predicted labels, where 1 (TRUE) represents a positive prediction and 0 (FALSE) represents a negative prediction.}

\item{truth}{A binary (logical) vector of true labels, where 1 (TRUE) represents a positive instance and 0 (FALSE) represents a negative instance.}
}
\value{
The precision of the classifier, as a numeric value between 0 and 1.
}
\description{
This function computes the precision (also known as positive predictive value) of a classifier.
Precision is the proportion of true positive predictions among all positive predictions made by the classifier (# True Positives / (# True Positives + # False Positives))..
}
\examples{
# Test the precision function
pred = c(1, 0, 1, 1, 0)
truth = c(1, 0, 1, 0, 1)
precision(pred, truth)

}
